{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/LeUdL9FhIKQ57trOdkLr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janShi1105/science/blob/main/ML17_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKwWK1n0ZqVc",
        "outputId": "4dec6945-363a-4b43-8112-18124db0cbe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/CPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "  device_name = tf.test.gpu_device_name()\n",
        "else:\n",
        "  device_name = '/CPU:0'\n",
        "print(device_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dcgan_generator(z_size=20, output_size= (28,28,1), n_filters=128, n_blocks=2):\n",
        "  size_factor = 2**n_blocks\n",
        "  hidden_size = (output_size[0]//size_factor, output_size[1]//size_factor)\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Input(shape=(z_size, )),\n",
        "      tf.keras.layers.Dense(units=n_filters * np.prod(hidden_size), use_bias = False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.LeakyReLU(),\n",
        "      tf.keras.layers.Reshape((hidden_size[0], hidden_size[1], n_filters)),\n",
        "      tf.keras.layers.Conv2DTranspose(filters=n_filters, kernel_size=(5,5), strides=(1,1), padding='same', use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.LeakyReLU()\n",
        "  ])\n",
        "\n",
        "  nf = n_filters\n",
        "  for i in range(n_blocks):\n",
        "    nf = nf //2\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(filters=nf, kernel_size=(5,5), strides=(2,2), padding='same', use_bias=False))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(filters=output_size[2], kernel_size=(5,5), strides=(1,1), padding='same', use_bias=False, activation='tanh'))\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "rKIxrXzYiCRF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dcgan_discriminator(input_size=(28,28,1), n_filters=64, n_blocks=2):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Input(shape=input_size), \n",
        "      tf.keras.layers.Conv2D(filters=n_filters, kernel_size=5, strides=(1,1), padding='same'),\n",
        "      tf.keras.layers.BatchNormalization(), \n",
        "      tf.keras.layers.LeakyReLU()\n",
        "  ])\n",
        "\n",
        "  nf = n_filters \n",
        "  for i in range(n_blocks):\n",
        "    nf = nf * 2\n",
        "    model.add(tf.keras.layers.Conv2D(filters=nf, kernel_size=(5,5), strides=(2,2), padding='same'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=(7,7), padding='valid'))\n",
        "  model.add(tf.keras.layers.Reshape((1,)))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "BPirhMtjjr_X"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "mnist_bldr = tfds.builder('mnist')\n",
        "mnist_bldr.download_and_prepare()\n",
        "mnist = mnist_bldr.as_dataset(shuffle_files=False)\n",
        "\n",
        "def preprocess(ex, mode='uniform'):\n",
        "  image = ex['image']\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  image = image * 2 - 1\n",
        "  if mode == 'uniform':\n",
        "    input_z = tf.random.uniform(shape=(z_size, ), minval=-1, maxval=1)\n",
        "  elif mode == 'normal':\n",
        "    input_z = tf.random.normal(shape=(z_size,))\n",
        "\n",
        "  return input_z, image"
      ],
      "metadata": {
        "id": "NJtb505UllCz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_model = make_dcgan_generator()\n",
        "gen_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T0yKygLpiNP",
        "outputId": "6c0e975b-da55-4fb9-9e09-aa9fbfb432f7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 6272)              125440    \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 6272)             25088     \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_14 (LeakyReLU)  (None, 6272)              0         \n",
            "                                                                 \n",
            " reshape_4 (Reshape)         (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_8 (Conv2DT  (None, 7, 7, 128)        409600    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 7, 7, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_15 (LeakyReLU)  (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_9 (Conv2DT  (None, 14, 14, 64)       204800    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 14, 14, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_16 (LeakyReLU)  (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_10 (Conv2D  (None, 28, 28, 32)       51200     \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 28, 28, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_17 (LeakyReLU)  (None, 28, 28, 32)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_11 (Conv2D  (None, 28, 28, 1)        800       \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 817,824\n",
            "Trainable params: 804,832\n",
            "Non-trainable params: 12,992\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "disc_model = make_dcgan_discriminator()\n",
        "disc_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1M6UZzlpnrj",
        "outputId": "d52984fd-0dfb-49e0-9aa7-d12fc32a27c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_8 (Conv2D)           (None, 28, 28, 64)        1664      \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 28, 28, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_18 (LeakyReLU)  (None, 28, 28, 64)        0         \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 14, 14, 128)       204928    \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 14, 14, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_19 (LeakyReLU)  (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 7, 7, 256)         819456    \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 7, 7, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_20 (LeakyReLU)  (None, 7, 7, 256)         0         \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 7, 7, 256)         0         \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 1, 1, 1)           12545     \n",
            "                                                                 \n",
            " reshape_5 (Reshape)         (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,040,385\n",
            "Trainable params: 1,039,489\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs  = 100\n",
        "batch_size = 128\n",
        "image_size = (28,28)\n",
        "z_size = 20\n",
        "mode_z = 'uniform'\n",
        "lambda_gp = 10\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "mnist_trainset = mnist['train']\n",
        "mnist_trainset = mnist_trainset.map(preprocess)\n",
        "mnist_trainset = mnist_trainset.shuffle(10000)\n",
        "mnist_trainset = mnist_trainset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "with tf.device(device_name):\n",
        "  gen_model = make_dcgan_generator()\n",
        "  gen_model.build(input_shape=(None, z_size))\n",
        "  disc_model = make_dcgan_discriminator()\n",
        "  disc_model.build(input_shape=(None, np.prod(image_size)))"
      ],
      "metadata": {
        "id": "bp_4WIQLpxZR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "g_optimizer = tf.keras.optimizers.Adam(0.0002)\n",
        "d_optimizer = tf.keras.optimizers.Adam(0.0002)\n",
        "\n",
        "if mode_z == 'uniform':\n",
        "  fixed_z = tf.random.uniform(shape=(batch_size, z_size), minval=-1, maxval =1)\n",
        "elif mode_z == 'normal':\n",
        "  fixed_z = tf.random.normal(shape=(batch_size, z_size))\n",
        "\n",
        "def create_sample(g_model, input_z):\n",
        "  g_output = g_model(input_z, training=False)\n",
        "  images = tf.reshape(g_output, (batch_size, *image_size))\n",
        "  return (images + 1)/2\n"
      ],
      "metadata": {
        "id": "PBUBJvZRu5y2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_losses = []\n",
        "epoch_samples = []\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "  epoch_losses  = []\n",
        "  for i, (input_z , input_real) in enumerate(mnist_trainset):\n",
        "    with tf.GradientTape() as d_tape, tf.GradientTape() as g_tape:\n",
        "      g_output = gen_model(input_z, training=True)\n",
        "      d_critics_real = disc_model(input_real, training = True)\n",
        "      d_critics_fake = disc_model(g_output, training =True)\n",
        "\n",
        "      g_loss = -tf.math.reduce_mean(d_critics_fake)\n",
        "\n",
        "      d_loss_real = -tf.math.reduce_mean(d_critics_real)\n",
        "      d_loss_fake = tf.math.reduce_mean(d_critics_fake)\n",
        "      d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "      with tf.GradientTape() as gp_tape:\n",
        "        alpha = tf.random.uniform(shape=[d_critics_real.shape[0], 1,1,1], minval=0, maxval=1)\n",
        "        interpolated = (alpha * input_real + (1-alpha) * g_output)\n",
        "        gp_tape.watch(interpolated)\n",
        "        d_critics_intp = disc_model(interpolated)\n",
        "\n",
        "      grads_intp = gp_tape.gradient(d_critics_intp, [interpolated,])[0]\n",
        "      grads_intp_l2 = tf.sqrt(tf.reduce_mean(tf.square(grads_intp), axis=[1,2,3]))\n",
        "      grad_penalty = tf.reduce_mean(tf.square(grads_intp_l2 -1))\n",
        "      d_loss = d_loss + lambda_gp * grad_penalty\n",
        "    d_grads = d_tape.gradient(d_loss, disc_model.trainable_variables)\n",
        "    d_optimizer.apply_gradients(\n",
        "        grads_and_vars=zip(d_grads, disc_model.trainable_variables)\n",
        "    )\n",
        "    g_grads = g_tape.gradient(g_loss, gen_model.trainable_variables)\n",
        "    g_optimizer.apply_gradients(grads_and_vars=zip(g_grads, gen_model.trainable_variables))\n",
        "    epoch_losses.append((g_loss.numpy(), d_loss.numpy(), d_loss_real.numpy(), d_loss_fake.numpy()))\n",
        "\n",
        "  all_losses.append(epoch_losses)\n",
        "  print('Epoch {:.3d} | ET {:.2f} min | Avg Losses >> G/D {:6.2f}/{:6.2f} [D-Real: {:6.2f}  D-Fake: {:6.2f}]'.format(epoch, (time.time() -start_time)/60, *list(np.mean(all_losses[-1], axis=0))))\n",
        "  epoch_samples.append(create_sample(gen_model, fixed_z).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "tNPxxebGvghJ",
        "outputId": "edc27bd4-3d58-4ed2-d16b-f37980b027b2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-1057e2ebcee8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0mgrad_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_intp_l2\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_gp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0md_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     d_optimizer.apply_gradients(\n\u001b[1;32m     31\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    154\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_FusedBatchNormV3Grad\u001b[0;34m(op, *grad)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegisterGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FusedBatchNormV3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_FusedBatchNormV3Grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_BaseFusedBatchNormGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_BaseFusedBatchNormGrad\u001b[0;34m(op, version, *grad)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reserve_space_3\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m     \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m     \u001b[0mpop_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mfused_batch_norm_grad_v3\u001b[0;34m(y_backprop, x, scale, reserve_space_1, reserve_space_2, reserve_space_3, epsilon, data_format, is_training, name)\u001b[0m\n\u001b[1;32m   4037\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FusedBatchNormGradV3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4038\u001b[0m         \u001b[0mreserve_space_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreserve_space_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreserve_space_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"epsilon\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4039\u001b[0;31m         \"data_format\", data_format, \"is_training\", is_training)\n\u001b[0m\u001b[1;32m   4040\u001b[0m       \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FusedBatchNormGradV3Output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4041\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "selected_epochs = [1,2,4,10,50,100]\n",
        "fig = plt.figure(figsize=(10,14))\n",
        "for i ,e in enumerate(selected_epochs):\n",
        "  for j in range(5):\n",
        "    ax = fig.add_subplot(6,5, i * 5 + j + 1)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    if j == 0:\n",
        "      ax.text(-0.06, 0.5, 'Epoch {}'.format(e), rotation=90, size=18, color='red', horizontalalignment='right', verticalalignment='center', transformAxes=ax.tranAxes)\n",
        "\n",
        "      image = epoch_samples[e-1][j]\n",
        "      ax.imshow(image, cmap='gray_r')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n4UjImAuyecV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GFLxEyCi1Aya"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}